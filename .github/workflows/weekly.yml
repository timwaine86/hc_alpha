name: weekly-insights

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * 1'   # Mondays 06:00 UTC

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas python-dateutil python-dotenv requests openai==1.51.2 tenacity

      - name: Debug Apify secrets
        shell: bash
        run: |
          echo "APIFY_TOKEN length: ${#APIFY_TOKEN:-0}"
          echo "APIFY_TASK_ID: ${APIFY_TASK_ID:-<empty>}"
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
          APIFY_TASK_ID: ${{ secrets.APIFY_TASK_ID }}

            - name: Run Apify Actor (posts only) and download fresh CSV
        shell: bash
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          set -e
          mkdir -p pipeline
          OUT="pipeline/latest_posts.csv"

          # First run: set a fixed catch-up date; afterwards change to a 7-day window.
          FROM_DATE="2025-10-01T00:00:00.000Z"
          # For rolling window later, use:
          # FROM_DATE="$(date -u -d '7 days ago' '+%Y-%m-%dT00:00:00.000Z')" || FROM_DATE="$(python - <<'PY'\nfrom datetime import datetime, timedelta, timezone\nprint((datetime.now(timezone.utc)-timedelta(days=7)).strftime('%Y-%m-%dT00:00:00.000Z'))\nPY)"

          DATA=$(cat <<JSON
          {
            "directUrls": ["https://www.instagram.com/masbymessi/"],
            "resultsLimit": 30,
            "scrapePosts": true,
            "fromDate": "$FROM_DATE"
          }
          JSON
          )

          # Call the official actor; include token in BOTH header and query (belt & braces)
          curl -fsSL -X POST \
            -H "Authorization: Bearer ${APIFY_TOKEN}" \
            -H "Content-Type: application/json" \
            "https://api.apify.com/v2/acts/apify~instagram-post-scraper/run-sync-get-dataset-items?format=csv&clean=true&token=${APIFY_TOKEN}" \
            -d "$DATA" \
            -o "$OUT"

          echo "First lines of downloaded CSV:"
          head -5 "$OUT"

      - name: Run baseline to Notion
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DB_MASPLUS: ${{ secrets.NOTION_DB_MASPLUS }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          BASELINE_ER_MEDIAN: '0.95'
          BASELINE_IVR_MEDIAN: '3.09'
          HC_INPUT_CSV: pipeline/latest_posts.csv
          HC_FOLLOWERS: '485000'
        run: python pipeline/baseline_check.py
