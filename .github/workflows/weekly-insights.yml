name: weekly-insights

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * 1'  # Mondays 06:00 UTC

permissions:
  contents: read

concurrency:
  group: weekly-insights-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  weekly_job:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas python-dateutil python-dotenv requests numpy openai==1.51.2 tenacity
          pip install yt-dlp
          sudo apt-get update
          sudo apt-get install -y ffmpeg

      # 1) Fetch MAS+ data from Apify (async poll)
      - name: Fetch MAS+ data from Apify (async poll)
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p pipeline
          OUT_RAW="pipeline/latest_posts_raw.csv"
          RUN_JSON="pipeline/apify_run.json"

          JSON_PAYLOAD='{"username":["masbymessi"],"maxItems":30,"proxy":{"useApifyProxy":true}}'

          curl -sS -X POST \
            -H "Authorization: Bearer ${APIFY_TOKEN}" \
            -H "Content-Type: application/json" \
            "https://api.apify.com/v2/acts/apify~instagram-post-scraper/runs?token=${APIFY_TOKEN}" \
            -d "${JSON_PAYLOAD}" \
            -o "${RUN_JSON}"

          RUN_ID=$(python -c 'import json,sys; d=json.load(open(sys.argv[1])); print(d.get("data",{}).get("id",""))' "${RUN_JSON}")
          [ -n "${RUN_ID}" ] || { echo "Failed to start Apify run"; sed -n '1,200p' "${RUN_JSON}"; exit 1; }
          echo "Apify run started: ${RUN_ID}"

          while :; do
            RESP=$(curl -sS -H "Authorization: Bearer ${APIFY_TOKEN}" "https://api.apify.com/v2/actor-runs/${RUN_ID}")
            STATUS=$(printf "%s" "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read()); print(d.get("data",{}).get("status",""))' || true)
            echo "  status: ${STATUS}"
            case "${STATUS}" in
              SUCCEEDED) break;;
              FAILED|TIMED-OUT|ABORTED) echo "${RESP}"; exit 1;;
            esac
            sleep 30
          done

          DATASET_ID=$(printf "%s" "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read()); print(d.get("data",{}).get("defaultDatasetId",""))')
          [ -n "${DATASET_ID}" ] || { echo "No dataset id"; echo "${RESP}"; exit 1; }

          curl -sS "https://api.apify.com/v2/datasets/${DATASET_ID}/items?format=csv&clean=true&token=${APIFY_TOKEN}" -o "${OUT_RAW}"
          [ -s "${OUT_RAW}" ] || { echo "Empty CSV"; exit 1; }
          echo "Raw CSV preview:"; head -5 "${OUT_RAW}"

      # 2) Normalize (robust shortCode + timestamp discovery)
      - name: Normalize CSV for baseline_check.py
        run: |
          python - <<'PY'
          import pandas as pd, numpy as np, re
          src="pipeline/latest_posts_raw.csv"; dst="pipeline/latest_posts.csv"
          df=pd.read_csv(src,dtype=str,keep_default_na=False)
          lower={c.lower():c for c in df.columns}
          def col(*names):
              for n in names:
                  if n and n.lower() in lower: return lower[n.lower()]
              return None
          sc_col=col("shortCode","short_code","shortcode")
          posturl_col=col("postUrl","url","permalink","link")
          if not sc_col and posturl_col:
              def extract_sc(u):
                  m=re.search(r"/(?:p|reel)/([A-Za-z0-9_-]+)/?", str(u))
                  return m.group(1) if m else ""
              df["__shortCode_from_url"]=df[posturl_col].map(extract_sc)
              sc_col="__shortCode_from_url"
          ts_candidates=["timestamp","takenAt","taken_at","takenAtISO","taken_at_iso","takenAtUtc","taken_at_utc",
                         "takenAtTimestamp","taken_at_timestamp","created_time","createdAt","published_time","date","time","datetime"]
          ts_cols=[c for c in (col(x) for x in ts_candidates) if c]
          def parse_ts_row(row):
              for c in ts_cols:
                  val=str(row.get(c,"")).strip()
                  if not val: continue
                  dt=pd.to_datetime(val,errors="coerce",utc=True)
                  if pd.notna(dt): return dt
                  num=pd.to_numeric(val,errors="coerce")
                  if pd.notna(num):
                      for unit in ["s","ms"]:
                          dt=pd.to_datetime(num,unit=unit,errors="coerce",utc=True)
                          if pd.notna(dt): return dt
              return pd.NaT
          df["_sc"]= (df[sc_col] if sc_col else "").astype(str).str.strip()
          df["_postUrl"]= df[posturl_col] if posturl_col else ""
          df["_dt"]= df.apply(parse_ts_row,axis=1)
          parent=df.loc[(df["_sc"].ne("")) & (df["_dt"].notna())].copy()
          if parent.empty:
              print("No parent rows with shortCode + timestamp found. Raw columns:", list(df.columns))
              pd.DataFrame(columns=["date_v4","shortCode","postUrl","type","caption","likesCount","commentsCount","views","displayUrl","videoUrl"]).to_csv(dst,index=False)
              raise SystemExit(0)
          parent["date_v4"]=parent["_dt"].dt.strftime("%Y-%m-%d")
          def first_nonempty(s):
              for v in s:
                  sv=str(v) if pd.notna(v) else ""
                  if sv.strip(): return sv
              return ""
          def colopt(*names): 
              c=col(*names)
              return c if c in parent.columns else None
          type_col=colopt("type","productType","media_type")
          cap_col =colopt("caption","caption_v4","caption_v3")
          like_col=colopt("likesCount","likes","like_count","edge_media_preview_like.count")
          com_col =colopt("commentsCount","comments","comment_count")
          vvc_col =colopt("videoViewCount","video_views","playCount","videoPlayCount")
          disp_col=colopt("displayUrl","imageUrl","thumbnailUrl","display_url")
          vid_col =colopt("videoUrl","video_url")
          rows=[]
          for sc,grp in parent.groupby("_sc"):
              row={"shortCode":sc,
                   "postUrl":first_nonempty(grp["_postUrl"]),
                   "caption":first_nonempty(grp[cap_col]) if cap_col else "",
                   "type":first_nonempty(grp[type_col]) if type_col else "",
                   "likesCount":first_nonempty(grp[like_col]) if like_col else "",
                   "commentsCount":first_nonempty(grp[com_col]) if com_col else "",
                   "displayUrl":first_nonempty(grp[disp_col]) if disp_col else "",
                   "videoUrl":first_nonempty(grp[vid_col]) if vid_col else "",
                   "date_v4":first_nonempty(grp["date_v4"]),
                   "__dt":grp["_dt"].max()}
              if vvc_col:
                  try:
                      vv=pd.to_numeric(grp[vvc_col].replace("",np.nan),errors="coerce").max()
                      row["views"]= "" if pd.isna(vv) else str(int(vv))
                  except: row["views"]=first_nonempty(grp[vvc_col])
              else: row["views"]=""
              rows.append(row)
          clean=pd.DataFrame(rows)
          clean["__dt"]=pd.to_datetime(clean["__dt"],errors="coerce",utc=True)
          clean=clean.sort_values("__dt",ascending=False).head(30).reset_index(drop=True)
          final_cols=["date_v4","shortCode","postUrl","type","caption","likesCount","commentsCount","views","displayUrl","videoUrl"]
          for c in final_cols:
              if c not in clean.columns: clean[c]=""
          clean=clean[final_cols]
          clean.to_csv(dst,index=False)
          print("Normalized columns:", list(clean.columns))
          print(clean.head(10).to_string(index=False))
          PY

      # 3) Media manifest (no downloads yet)
      - name: Create media manifest (no downloads yet)
        run: |
          python - <<'PY'
          import pandas as pd, re
          raw=pd.read_csv("pipeline/latest_posts_raw.csv")
          clean=pd.read_csv("pipeline/latest_posts.csv")
          def find(cols,pat): return [c for c in cols if re.search(pat,c,re.I)]
          url_cols=find(raw.columns,r'(display|image|thumb|media|video).*url$|^url$')
          type_cols=find(raw.columns,r'(is|media).*video|product.*type|type$')
          id_cols=find(raw.columns,r'shortcode$|^short[_]?code$|postid$|id$')
          id_col=id_cols[0] if id_cols else None
          media_type=None
          if type_cols:
              t=raw[type_cols[0]].astype(str).str.lower()
              media_type=t.map(lambda x:'video' if ('true' in x or 'video' in x) else 'image')
          out={'post_id':raw[id_col] if id_col else pd.Series(range(len(raw))),
               'media_type': media_type if media_type is not None else pd.Series(['unknown']*len(raw))}
          for c in url_cols: out[c]=raw[c]
          out['shortCode']=clean.get('shortCode')
          pd.DataFrame(out).to_csv("pipeline/media_manifest.csv",index=False)
          PY

      # 4) Download videos: try direct videoUrl first; if not enough, fall back to yt-dlp with headers/cookies; extract 10 frames each
      - name: Download IG videos and extract 10 frames each
        env:
          IG_SESSIONID:   ${{ secrets.IG_SESSIONID }}
          IG_DS_USER_ID:  ${{ secrets.IG_DS_USER_ID }}
          IG_CSRF:        ${{ secrets.IG_CSRF }}
          IG_RUR:         ${{ secrets.IG_RUR }}
          IG_MID:         ${{ secrets.IG_MID }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, time, shlex, subprocess, pandas as pd, requests

          os.makedirs("media_video", exist_ok=True)
          os.makedirs("frames_video", exist_ok=True)

          df = pd.read_csv("pipeline/latest_posts.csv")

          # ---- 1) Try direct CDN download from videoUrl (fastest, no cookie) ----
          direct = []
          s = requests.Session()
          headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0 Safari/537.36",
                     "Referer": "https://www.instagram.com/"}

          for _, r in df.iterrows():
            vu = str(r.get("videoUrl") or "").strip()
            sc = str(r.get("shortCode") or "").strip()
            if not vu or not sc:
              continue
            try:
              with s.get(vu, headers=headers, stream=True, timeout=45) as resp:
                ct = (resp.headers.get("Content-Type") or "").lower()
                if resp.status_code == 200 and "video/mp4" in ct:
                  out = f"media_video/{sc}.mp4"
                  with open(out, "wb") as f:
                    for chunk in resp.iter_content(8192):
                      if chunk: f.write(chunk)
                  print("Direct saved:", out)
                  direct.append(out)
            except Exception as e:
              print("Direct download failed for", sc, e)
            if len(direct) >= 10:
              break

          # ---- 2) If fewer than 10, try yt-dlp with Cookie+UA+Referer on postUrl ----
          target = 10
          downloaded = list(direct)

          # Build Cookie header from whatever we have
          import os
          cookie_parts = []
          for name, envkey in [("sessionid","IG_SESSIONID"),
                               ("ds_user_id","IG_DS_USER_ID"),
                               ("csrftoken","IG_CSRF"),
                               ("rur","IG_RUR"),
                               ("mid","IG_MID")]:
            val = os.environ.get(envkey,"").strip()
            if val:
              cookie_parts.append(f"{name}={val}")
          cookie_hdr = "; ".join(cookie_parts)

          ua = "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0 Safari/537.36"
          add_headers = []
          add_headers += ["--add-header", f"User-Agent:{ua}"]
          add_headers += ["--add-header", "Referer:https://www.instagram.com/"]
          if cookie_hdr:
            add_headers += ["--add-header", f"Cookie:{cookie_hdr}"]

          # Prioritise likely videos (reel/video)
          candidates = []
          seen = set()
          for _, r in df.iterrows():
            u = str(r.get("postUrl") or "").strip()
            if not u or u in seen:
              continue
            score = 0
            if "/reel/" in u: score += 2
            score += 1  # at least try it
            candidates.append((score, u))
            seen.add(u)
          candidates.sort(key=lambda x: x[0], reverse=True)
          urls = [u for _,u in candidates][:120]  # scan deeper

          def try_ytdlp(u, tries=2, sleep_s=2):
            base = ["yt-dlp", "--no-playlist", "-S", "ext:mp4:m4a", "--merge-output-format", "mp4",
                    "--no-keep-video", "--no-keep-fragments", "--no-warnings",
                    "-o", "media_video/%(id)s.%(ext)s"]
            cmd = base + add_headers + [u]
            for k in range(tries):
              print(f"[yt-dlp {k+1}/{tries}] {u}")
              rc = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL).returncode
              if rc == 0:
                mp4s = sorted([f for f in os.listdir('media_video') if f.endswith('.mp4')],
                              key=lambda x: os.path.getmtime(os.path.join('media_video',x)), reverse=True)
                if mp4s:
                  return os.path.join('media_video', mp4s[0])
              time.sleep(sleep_s)
            return None

          for u in urls:
            if len(downloaded) >= target:
              break
            path = try_ytdlp(u)
            if path and path not in downloaded:
              downloaded.append(path)
              print("yt-dlp saved:", path)

          # Clean up non-mp4 left-overs
          for f in list(os.listdir('media_video')):
            if not f.endswith('.mp4'):
              try: os.remove(os.path.join('media_video', f))
              except: pass

          if not downloaded:
            print("No videos saved; skipping frames.")
            raise SystemExit(0)

          # ---- 3) Extract 10 evenly spaced frames per video ----
          def ffprobe_duration(path):
            try:
              out = subprocess.check_output(
                ["ffprobe","-v","error","-select_streams","v:0","-show_entries","stream=duration","-of","default=nw=1:nk=1",path]
              ).decode().strip()
              return float(out) if out else None
            except Exception:
              return None

          for vid in downloaded:
            base = os.path.splitext(os.path.basename(vid))[0]
            dur = ffprobe_duration(vid)
            if not dur or dur <= 0:
              print("Could not probe duration for", vid); continue
            stamps = [(i+1)*dur/11 for i in range(10)]
            for idx,t in enumerate(stamps,1):
              out = f"frames_video/{base}_frame_{idx:02d}.jpg"
              subprocess.run(
                ["ffmpeg","-y","-ss",f"{t:.3f}","-i",vid,"-frames:v","1","-q:v","2",out],
                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
              )
              print("Saved frame:", out)
          PY

      # 5) Upload artifacts (CSVs, manifest, videos, frames)
      - name: Upload artifacts (csvs, manifest, videos, frames)
        uses: actions/upload-artifact@v4
        with:
          name: apify_csvs_media_and_frames
          path: |
            pipeline/latest_posts_raw.csv
            pipeline/latest_posts.csv
            pipeline/media_manifest.csv
            media_video
            frames_video

      # 6) (Optional) Baseline to Notion
      - name: Run baseline to Notion
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DB_MASPLUS: ${{ secrets.NOTION_DB_MASPLUS }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          BASELINE_ER_MEDIAN: '0.95'
          BASELINE_IVR_MEDIAN: '3.09'
          HC_INPUT_CSV: pipeline/latest_posts.csv
          HC_FOLLOWERS: '485000'
        run: python pipeline/baseline_check.py
