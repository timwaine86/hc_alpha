name: weekly-insights

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * 1'  # Mondays 06:00 UTC

permissions:
  contents: read

concurrency:
  group: weekly-insights-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  weekly_job:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas python-dateutil python-dotenv requests numpy tenacity openai==1.51.2
          pip install yt-dlp
          sudo apt-get update
          sudo apt-get install -y ffmpeg

      # -----------------------------
      # 1) Fetch from Apify (async)
      # -----------------------------
      - name: Fetch MAS+ data from Apify (async poll)
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p pipeline
          OUT_RAW="pipeline/latest_posts_raw.csv"
          RUN_JSON="pipeline/apify_run.json"

          JSON_PAYLOAD='{"username":["masbymessi"],"maxItems":100,"proxy":{"useApifyProxy":true}}'

          curl -sS -X POST \
            -H "Authorization: Bearer ${APIFY_TOKEN}" \
            -H "Content-Type: application/json" \
            "https://api.apify.com/v2/acts/apify~instagram-post-scraper/runs?token=${APIFY_TOKEN}" \
            -d "${JSON_PAYLOAD}" \
            -o "${RUN_JSON}"

          RUN_ID=$(python -c 'import json,sys; d=json.load(open(sys.argv[1])); print(d.get("data",{}).get("id",""))' "${RUN_JSON}")
          if [ -z "${RUN_ID}" ]; then
            echo "Failed to start Apify run"
            sed -n '1,200p' "${RUN_JSON}" || true
            exit 1
          fi

          while :; do
            RESP=$(curl -sS -H "Authorization: Bearer ${APIFY_TOKEN}" "https://api.apify.com/v2/actor-runs/${RUN_ID}")
            STATUS=$(printf '%s' "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read() or "{}"); print(d.get("data",{}).get("status",""))')
            echo "status: ${STATUS}"
            case "${STATUS}" in
              SUCCEEDED) break;;
              FAILED|TIMED-OUT|ABORTED) printf '%s\n' "${RESP}"; exit 1;;
            esac
            sleep 30
          done

          DATASET_ID=$(printf '%s' "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read() or "{}"); print(d.get("data",{}).get("defaultDatasetId",""))')
          if [ -z "${DATASET_ID}" ]; then
            echo "No dataset id"
            printf '%s\n' "${RESP}"
            exit 1
          fi

          curl -sS "https://api.apify.com/v2/datasets/${DATASET_ID}/items?format=csv&clean=true&token=${APIFY_TOKEN}" -o "${OUT_RAW}"
          if [ ! -s "${OUT_RAW}" ]; then
            echo "Empty CSV"
            exit 1
          fi
          head -5 "${OUT_RAW}"

      # --------------------------------------------
      # 2) Normalize -> last 7 days, tidy columns
      # --------------------------------------------
      - name: Normalize CSV for baseline_check.py
        run: |
          python - <<'PY'
          import pandas as pd, numpy as np, re
          from pandas import Timestamp, Timedelta

          src="pipeline/latest_posts_raw.csv"; dst="pipeline/latest_posts.csv"
          df=pd.read_csv(src,dtype=str,keep_default_na=False)
          lower={c.lower():c for c in df.columns}
          def col(*names):
              for n in names:
                  if n and n.lower() in lower: return lower[n.lower()]
              return None

          sc_col=col("shortCode","short_code","shortcode")
          posturl_col=col("postUrl","url","permalink","link")
          if not sc_col and posturl_col:
              def extract_sc(u):
                  m=re.search(r"/(?:p|reel)/([A-Za-z0-9_-]+)/?", str(u)); return m.group(1) if m else ""
              df["__sc_from_url"]=df[posturl_col].map(extract_sc); sc_col="__sc_from_url"

          ts_candidates=["timestamp","takenAt","taken_at","takenAtISO","taken_at_iso","takenAtUtc","taken_at_utc",
                         "takenAtTimestamp","taken_at_timestamp","created_time","createdAt","published_time","date","time","datetime"]
          ts_cols=[c for c in (col(x) for x in ts_candidates) if c]

          def parse_ts_row(row):
              for c in ts_cols:
                  val=str(row.get(c,"")).strip()
                  if not val: continue
                  dt=pd.to_datetime(val,errors="coerce",utc=True)
                  if pd.notna(dt): return dt
                  num=pd.to_numeric(val,errors="coerce")
                  if pd.notna(num):
                      for unit in ["s","ms"]:
                          dt=pd.to_datetime(num,unit=unit,errors="coerce",utc=True)
                          if pd.notna(dt): return dt
              return pd.NaT

          df["_sc"]= (df[sc_col] if sc_col else "").astype(str).str.strip()
          df["_postUrl"]= df[posturl_col] if posturl_col else ""
          df["_dt"]= df.apply(parse_ts_row,axis=1)

          now = Timestamp.now(tz="UTC"); week_cut = now - Timedelta(days=7)
          df = df.loc[df["_dt"].notna() & (df["_dt"] >= week_cut)].copy()

          parent=df.loc[df["_sc"].ne("")].copy()
          if parent.empty:
              pd.DataFrame(columns=["date_v4","shortCode","postUrl","type","caption","likesCount","commentsCount","views","displayUrl","videoUrl"]).to_csv(dst,index=False)
              raise SystemExit(0)

          parent["date_v4"]=parent["_dt"].dt.strftime("%Y-%m-%d")

          def first_nonempty(s):
              for v in s:
                  sv=str(v) if pd.notna(v) else ""
                  if sv.strip(): return sv
              return ""

          def colopt(*names):
              c=col(*names); return c if (c in parent.columns) else None

          type_col=colopt("type","productType","media_type")
          cap_col =colopt("caption","caption_v4","caption_v3")
          like_col=colopt("likesCount","likes","like_count","edge_media_preview_like.count")
          com_col =colopt("commentsCount","comments","comment_count")
          vvc_col =colopt("videoViewCount","video_views","playCount","videoPlayCount")
          disp_col=colopt("displayUrl","imageUrl","thumbnailUrl","display_url")
          vid_col =colopt("videoUrl","video_url")

          rows=[]
          for sc,grp in parent.groupby("_sc"):
              row={"shortCode":sc,
                   "postUrl":first_nonempty(grp["_postUrl"]),
                   "caption":first_nonempty(grp[cap_col]) if cap_col else "",
                   "type":first_nonempty(grp[type_col]) if type_col else "",
                   "likesCount":first_nonempty(grp[like_col]) if like_col else "",
                   "commentsCount":first_nonempty(grp[com_col]) if com_col else "",
                   "displayUrl":first_nonempty(grp[disp_col]) if disp_col else "",
                   "videoUrl":first_nonempty(grp[vid_col]) if vid_col else "",
                   "date_v4":first_nonempty(grp["date_v4"]),
                   "__dt":grp["_dt"].max()}
              if vvc_col:
                  try:
                      vv=pd.to_numeric(grp[vvc_col].replace("",np.nan),errors="coerce").max()
                      row["views"]= "" if pd.isna(vv) else str(int(vv))
                  except: row["views"]=first_nonempty(grp[vvc_col])
              else: row["views"]=""
              rows.append(row)

          clean=pd.DataFrame(rows)
          clean["__dt"]=pd.to_datetime(clean["__dt"],errors="coerce",utc=True)
          clean=clean.sort_values("__dt",ascending=False).reset_index(drop=True)

          final_cols=["date_v4","shortCode","postUrl","type","caption","likesCount","commentsCount","views","displayUrl","videoUrl"]
          for c in final_cols:
              if c not in clean.columns: clean[c]=""
          clean=clean[final_cols]
          clean.to_csv(dst,index=False)
          print("Weekly rows:", len(clean))
          PY

      # ----------------------------------------------------
      # 3) Videos -> <shortCode>.mp4 ; frames -> per post
      # ----------------------------------------------------
      - name: Download IG videos and extract 10 frames each
        env:
          IG_SESSIONID:   ${{ secrets.IG_SESSIONID }}
          IG_DS_USER_ID:  ${{ secrets.IG_DS_USER_ID }}
          IG_CSRF:        ${{ secrets.IG_CSRF }}
          IG_RUR:         ${{ secrets.IG_RUR }}
          IG_MID:         ${{ secrets.IG_MID }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, time, subprocess, pandas as pd, requests

          os.makedirs("media_video", exist_ok=True)
          os.makedirs("frames_video", exist_ok=True)

          df=pd.read_csv("pipeline/latest_posts.csv")

          cookie_names=[("sessionid","IG_SESSIONID"),("ds_user_id","IG_DS_USER_ID"),
                        ("csrftoken","IG_CSRF"),("rur","IG_RUR"),("mid","IG_MID")]
          parts=[]
          import os as _os
          for name,envk in cookie_names:
            val=_os.environ.get(envk,"").strip()
            if val: parts.append(f"{name}={val}")
          COOKIE="; ".join(parts)
          UA="Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0 Safari/537.36"
          REF="https://www.instagram.com/"

          s=requests.Session()
          s.headers.update({"User-Agent":UA,"Referer":REF})
          if COOKIE: s.headers.update({"Cookie":COOKIE})

          downloaded=[]

          # try direct CDN first
          for _,r in df.iterrows():
            sc=str(r["shortCode"]); vu=str(r.get("videoUrl","") or "")
            if not sc or not vu: continue
            out=f"media_video/{sc}.mp4"
            if os.path.exists(out): downloaded.append(out); continue
            try:
              with s.get(vu, stream=True, timeout=45) as resp:
                ct=(resp.headers.get("Content-Type") or "").lower()
                if resp.status_code==200 and "video/mp4" in ct:
                  with open(out,"wb") as f:
                    for chunk in resp.iter_content(8192):
                      if chunk: f.write(chunk)
                  print("Direct saved:", out); downloaded.append(out)
            except Exception as e:
              print("Direct failed for", sc, e)

          # yt-dlp fallback (3 tries per post)
          def ytdlp(sc,url):
            base=["yt-dlp","--no-playlist","-S","ext:mp4:m4a","--merge-output-format","mp4",
                  "--no-keep-video","--no-keep-fragments","--no-warnings",
                  "-o", f"media_video/{sc}.%(ext)s",
                  "--add-header", f"User-Agent:{UA}",
                  "--add-header", f"Referer:{REF}"]
            if COOKIE: base+=["--add-header", f"Cookie:{COOKIE}"]
            rc=subprocess.run(base+[url], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL).returncode
            return rc==0 and os.path.exists(f"media_video/{sc}.mp4")

          for _,r in df.iterrows():
            sc=str(r["shortCode"]); url=str(r.get("postUrl","") or "")
            if not sc or not url: continue
            if os.path.exists(f"media_video/{sc}.mp4"): continue
            for _ in range(3):
              if ytdlp(sc,url):
                downloaded.append(f"media_video/{sc}.mp4"); break
              time.sleep(2)

          # clean non-mp4
          for f in list(os.listdir("media_video")):
            if not f.endswith(".mp4"):
              try: os.remove(os.path.join("media_video",f))
              except: pass

          if not downloaded:
            print("No videos saved; skipping frames."); raise SystemExit(0)

          # extract frames
          def dur(path):
            try:
              out=subprocess.check_output(
                ["ffprobe","-v","error","-select_streams","v:0","-show_entries","stream=duration","-of","default=nw=1:nk=1",path]
              ).decode().strip()
              return float(out) if out else None
            except: return None

          for vid in downloaded:
            sc=os.path.splitext(os.path.basename(vid))[0]
            d=dur(vid)
            if not d or d<=0: print("No duration for", vid); continue
            tgt=os.path.join("frames_video",sc); os.makedirs(tgt,exist_ok=True)
            stamps=[(i+1)*d/11 for i in range(10)]
            for i,t in enumerate(stamps,1):
              out=os.path.join(tgt,f"frame_{i:02d}.jpg")
              subprocess.run(["ffmpeg","-y","-ss",f"{t:.3f}","-i",vid,"-frames:v","1","-q:v","2",out],
                             stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
              print("Saved", out)
          PY

      # --------------------------------------------------------------
      # 4) OpenAI: Motif (Vision) + Caption Intent + Territory -> CSV
      # --------------------------------------------------------------
      - name: Classify motifs + caption intent + territory (OpenAI)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, base64, glob, json, re, pandas as pd, httpx
          from tenacity import retry, stop_after_attempt, wait_exponential
          from openai import OpenAI

          http_client = httpx.Client(trust_env=False, timeout=60)
          client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), http_client=http_client)

          posts = pd.read_csv("pipeline/latest_posts.csv")

          MOTIF_SYSTEM = """You classify social video frames into ONE visual motif:
          1) Product Macro — close-up product/bottle, clean framing.
          2) Lifestyle / Environment — everyday routine scenes (desk, commute, gym bag).
          3) Crowd POV — crowds, event/stadium energy, spectacle.
          4) Creator POV — handheld/interview/training/prep.
          Return ONLY JSON: {"motif":"<label>","confidence":0-1,"reason":"<12 words>"}."""

          def enc_img(path):
            with open(path,"rb") as f:
              b64=base64.b64encode(f.read()).decode("utf-8")
            return {"type":"input_image","image_url":{"url":f"data:image/jpeg;base64,{b64}"}}

          @retry(stop=stop_after_attempt(3), wait=wait_exponential(1,1,6))
          def classify_motif(frames):
            content=[{"type":"input_text","text":MOTIF_SYSTEM}]
            if frames:
              idxs=[0, max(0,len(frames)//4), len(frames)//2, min(len(frames)-1, 3*len(frames)//4), len(frames)-1]
              for i in sorted(set(idxs)): content.append(enc_img(frames[i]))
            resp=client.chat.completions.create(model="gpt-4o-mini", messages=[{"role":"user","content":content}], temperature=0)
            return resp.choices[0].message.content.strip()

          TERRITORY_DEFS = """Classify a MAS+ Instagram post into ONE territory and ONE intent.
          Territories: Football / Messi Baseline; Future Clarity; Performance Mastery; Creative Momentum.
          Intents: Participation / Prompt; Focus / Discipline; Product / Availability; Celebration / Moment.
          Return ONLY JSON: {"territory":"<one>","intent":"<one>"}."""

          @retry(stop=stop_after_attempt(3), wait=wait_exponential(1,1,6))
          def classify_text(caption, motif_hint):
            prompt=f"{TERRITORY_DEFS}\nMotif hint: {motif_hint}\nCaption: {caption}"
            resp=client.chat.completions.create(model="gpt-4o-mini", messages=[{"role":"user","content":prompt}], temperature=0)
            return resp.choices[0].message.content.strip()

          rows=[]
          for _, r in posts.iterrows():
            sc=str(r.get("shortCode") or "")
            frames=sorted(glob.glob(os.path.join("frames_video",sc,"frame_*.jpg")))
            motif_json='{"motif":"Lifestyle / Environment","confidence":0.5,"reason":"no frames"}'
            if frames:
              try: motif_json=classify_motif(frames)
              except Exception as e: print("motif error", sc, e)
            try:
              mj=json.loads(motif_json); 
              if isinstance(mj,str): mj=json.loads(mj)
              motif_label=mj.get("motif","Lifestyle / Environment"); motif_conf=mj.get("confidence",0.5)
            except Exception:
              m=re.search(r'"motif"\s*:\s*"([^"]+)"', motif_json); motif_label=m.group(1) if m else "Lifestyle / Environment"; motif_conf=0.5
            try:
              text_json=classify_text(r.get("caption",""), motif_label)
              tj=json.loads(text_json); 
              if isinstance(tj,str): tj=json.loads(tj)
              terr=tj.get("territory","Future Clarity"); intent=tj.get("intent","Celebration / Moment")
            except Exception:
              terr="Future Clarity"; intent="Celebration / Moment"
            rows.append({"shortCode":sc,"Motif_v1":motif_label,"Motif_conf":motif_conf,"Caption_Intent_v1":intent,"Assigned_Territory_v4":terr})

          add=pd.DataFrame(rows)
          merged=posts.merge(add, on="shortCode", how="left")
          merged.to_csv("pipeline/latest_posts.csv", index=False)
          print("Merged into latest_posts.csv")
          PY

      # -------------------------------------------------------
      # 5) Notion: populate all requested properties on a page
      # -------------------------------------------------------
      - name: Publish weekly visual narrative to Notion
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DB_MASPLUS: ${{ secrets.NOTION_DB_MASPLUS }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, json, pandas as pd, requests
          from datetime import datetime, timezone

          NOTION_TOKEN=os.environ["NOTION_TOKEN"]
          DB_ID=os.environ["NOTION_DB_MASPLUS"]
          H={"Authorization":f"Bearer {NOTION_TOKEN}","Notion-Version":"2022-06-28","Content-Type":"application/json"}

          def rt(x):  return {"rich_text":[{"type":"text","text":{"content":str(x)[:1900]}}]}
          def ttl(x): return {"title":[{"type":"text","text":{"content":str(x)[:200]}}]}
          def sel(x): return {"select":{"name":x}}
          def msel(tags): return {"multi_select":[{"name":t[:100]} for t in tags if t]}

          posts=pd.read_csv("pipeline/latest_posts.csv") if os.path.exists("pipeline/latest_posts.csv") else pd.DataFrame()
          week=datetime.now(timezone.utc).strftime("%Y-%m-%d")
          n=len(posts)

          motif_counts=posts.get("Motif_v1",pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
          terr_counts =posts.get("Assigned_Territory_v4",pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
          motif_line=" • ".join([f"{k}: {v}" for k,v in motif_counts.items()]) if motif_counts else "No motifs"
          terr_line =" • ".join([f"{k}: {v}" for k,v in terr_counts.items()])  if terr_counts  else "No territories"
          metric_text=f"Posts: {n} | Motifs: {motif_line} | Territories: {terr_line}"

          avg_conf=float(posts.get("Motif_conf",pd.Series([0.5]*n)).mean()) if n else 0.5
          conf_band="High" if avg_conf>=0.75 else ("Medium" if avg_conf>=0.5 else "Low")

          motifs=[m for m in posts.get("Motif_v1",[]) if isinstance(m,str)]
          intents=[i for i in posts.get("Caption_Intent_v1",[]) if isinstance(i,str)]
          driver_tags=sorted(set(motifs+intents))

          def row_line(r): return f"{r.get('date_v4','?')} · {r.get('shortCode','?')} · {r.get('postUrl','')}"
          linked_posts="\n".join([row_line(r) for _,r in posts.iterrows()]) if n else ""

          top_motif=max(motif_counts,key=motif_counts.get) if motif_counts else None
          top_terr =max(terr_counts, key=terr_counts.get)  if terr_counts  else None
          if top_motif=="Crowd POV":
            action_text="Follow crowd-led posts with a Product Macro within 24h to carry meaning."
          elif top_motif=="Product Macro":
            action_text="Keep macro cadence; pair with Creator POV to humanise routine."
          elif top_motif=="Creator POV":
            action_text="Use creator-led short cuts to bridge hype → habit."
          else:
            action_text="Balance reach (Crowd/Creative Momentum) with depth (Macro/Performance)."
          if top_terr:
            action_text+=f" Territory signal strongest in: {top_terr}."

          coverage=min(1.0, n/10.0)
          impact_score=round((coverage*avg_conf)*100)

          dbg="\n".join([f"{r.get('shortCode','?')}: {r.get('Motif_conf',0)}" for _,r in posts.iterrows()]) if n else ""

          payload={
            "parent":{"database_id":DB_ID},
            "properties":{
              "Date":{"date":{"start":week}},
              "Insight Type":sel("Weekly Motifs"),
              "Title":ttl("Weekly Motifs & Territories"),
              "Headline":rt(f"Weekly Motifs & Territories — {week}"),
              "Metric":rt(metric_text),
              "Confidence":sel(conf_band),
              "Driver Tags":msel(driver_tags),
              "Linked Posts":rt(linked_posts),
              "Action":rt(action_text),
              "Impact Score":{"number":impact_score},
              "Debug Confidence":rt(dbg),
              "Status":sel("Published")
            },
            "children":[
              {"object":"block","type":"paragraph",
               "paragraph":{"rich_text":[{"type":"text","text":{"content":metric_text}}]}}
            ]
          }

          r=requests.post("https://api.notion.com/v1/pages", headers=H, data=json.dumps(payload))
          print("Notion:", r.status_code, r.text[:300])
          PY

      # -----------------------
      # 6) Upload artifacts
      # -----------------------
      - name: Upload weekly artifacts
        uses: actions/upload-artifact@v4
        with:
          name: weekly_csvs_videos_frames
          path: |
            pipeline/latest_posts_raw.csv
            pipeline/latest_posts.csv
            media_video
            frames_video
