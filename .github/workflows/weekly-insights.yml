name: weekly-insights

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * 1'  # Mondays 06:00 UTC

permissions:
  contents: read

concurrency:
  group: weekly-insights-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  weekly_job:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas python-dateutil python-dotenv requests openai==1.51.2 tenacity

      - name: Fetch MAS+ data from Apify (async poll)
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p pipeline
          OUT_RAW="pipeline/latest_posts_raw.csv"
          RUN_JSON="pipeline/apify_run.json"
          JSON_PAYLOAD='{"username":["masbymessi"],"maxItems":30,"proxy":{"useApifyProxy":true}}'
          curl -sS -X POST -H "Authorization: Bearer ${APIFY_TOKEN}" -H "Content-Type: application/json" \
            "https://api.apify.com/v2/acts/apify~instagram-post-scraper/runs?token=${APIFY_TOKEN}" \
            -d "${JSON_PAYLOAD}" -o "${RUN_JSON}"
          RUN_ID=$(python -c 'import json,sys; d=json.load(open(sys.argv[1])); print(d.get("data",{}).get("id",""))' "${RUN_JSON}")
          [ -n "${RUN_ID}" ] || { echo "Failed to start Apify run"; sed -n '1,200p' "${RUN_JSON}"; exit 1; }
          while :; do
            RESP=$(curl -sS -H "Authorization: Bearer ${APIFY_TOKEN}" "https://api.apify.com/v2/actor-runs/${RUN_ID}")
            STATUS=$(printf "%s" "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read()); print(d.get("data",{}).get("status",""))' || true)
            echo "status: ${STATUS}"
            [ "${STATUS}" = "SUCCEEDED" ] && break
            case "${STATUS}" in FAILED|TIMED-OUT|ABORTED) echo "${RESP}"; exit 1;; esac
            sleep 30
          done
          DATASET_ID=$(printf "%s" "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read()); print(d.get("data",{}).get("defaultDatasetId",""))')
          [ -n "${DATASET_ID}" ] || { echo "No dataset id"; echo "${RESP}"; exit 1; }
          curl -sS "https://api.apify.com/v2/datasets/${DATASET_ID}/items?format=csv&clean=true&token=${APIFY_TOKEN}" -o "${OUT_RAW}"
          [ -s "${OUT_RAW}" ] || { echo "Empty CSV"; exit 1; }
          echo "Raw CSV preview:"; head -5 "${OUT_RAW}"

      - name: Normalize CSV for baseline_check.py
        run: |
          python - <<'PY'
          import pandas as pd
          src="pipeline/latest_posts_raw.csv"; dst="pipeline/latest_posts.csv"
          df=pd.read_csv(src)
          lower={c.lower():c for c in df.columns}
          candidates=["date_v4","date_v3","date","timestamp","takenat","taken_at","published_time","takenatiso","created_time"]
          col=next((lower[c] for c in candidates if c in lower),None)
          if not col: raise SystemExit(f"No date-like column. Columns: {list(df.columns)}")
          s=df[col]; d=pd.to_datetime(s,errors="coerce",utc=True)
          if d.isna().all():
            d=pd.to_datetime(pd.to_numeric(s,errors="coerce"),unit="s",utc=True)
            if d.isna().all():
              d=pd.to_datetime(pd.to_numeric(s,errors="coerce"),unit="ms",utc=True)
          if d.isna().all(): raise SystemExit(f"Could not parse dates from {col}")
          df["date_v4"]=d.dt.strftime("%Y-%m-%d")
          ren={}
          if "shortcode" in lower: ren[lower["shortcode"]]="shortCode"
          if "short_code" in lower: ren[lower["short_code"]]="shortCode"
          if "url" in lower: ren[lower["url"]]="postUrl"
          df=df.rename(columns=ren)
          df.to_csv(dst,index=False)
          print("Normalized columns:",list(df.columns)); print(df.head(5).to_string())
          PY

      - name: Show baseline_check.py header (debug)
        run: |
          echo "== printing first 60 lines of pipeline/baseline_check.py =="
          sed -n '1,60p' pipeline/baseline_check.py

      - name: Run baseline to Notion
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DB_MASPLUS: ${{ secrets.NOTION_DB_MASPLUS }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          BASELINE_ER_MEDIAN: '0.95'
          BASELINE_IVR_MEDIAN: '3.09'
          HC_INPUT_CSV: pipeline/latest_posts.csv
          HC_FOLLOWERS: '485000'
        run: python pipeline/baseline_check.py
