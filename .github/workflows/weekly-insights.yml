name: weekly-insights

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * 1'  # Mondays 06:00 UTC

permissions:
  contents: read

concurrency:
  group: weekly-insights-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  weekly_job:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas python-dateutil python-dotenv requests openai==1.51.2 tenacity numpy

      # 1) Start Apify actor (async) -> poll until SUCCEEDED -> download CSV
      - name: Fetch MAS+ data from Apify (async poll)
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p pipeline
          OUT_RAW="pipeline/latest_posts_raw.csv"
          RUN_JSON="pipeline/apify_run.json"

          JSON_PAYLOAD='{"username":["masbymessi"],"maxItems":30,"proxy":{"useApifyProxy":true}}'

          # Start run (async)
          curl -sS -X POST \
            -H "Authorization: Bearer ${APIFY_TOKEN}" \
            -H "Content-Type: application/json" \
            "https://api.apify.com/v2/acts/apify~instagram-post-scraper/runs?token=${APIFY_TOKEN}" \
            -d "${JSON_PAYLOAD}" \
            -o "${RUN_JSON}"

          RUN_ID=$(python -c 'import json,sys; d=json.load(open(sys.argv[1])); print(d.get("data",{}).get("id",""))' "${RUN_JSON}")
          if [ -z "${RUN_ID}" ]; then
            echo "Failed to start Apify run."
            sed -n '1,200p' "${RUN_JSON}" || true
            exit 1
          fi
          echo "Apify run started: ${RUN_ID}"

          # Poll until SUCCEEDED
          while :; do
            RESP=$(curl -sS -H "Authorization: Bearer ${APIFY_TOKEN}" "https://api.apify.com/v2/actor-runs/${RUN_ID}")
            STATUS=$(printf "%s" "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read()); print(d.get("data",{}).get("status",""))' || true)
            echo "  status: ${STATUS}"
            if [ "${STATUS}" = "SUCCEEDED" ]; then
              break
            elif [ "${STATUS}" = "FAILED" ] || [ "${STATUS}" = "TIMED-OUT" ] || [ "${STATUS}" = "ABORTED" ]; then
              echo "Run ended with status: ${STATUS}"
              printf "%s\n" "${RESP}" | sed -n '1,200p'
              exit 1
            fi
            sleep 30
          done

          DATASET_ID=$(printf "%s" "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read()); print(d.get("data",{}).get("defaultDatasetId",""))')
          if [ -z "${DATASET_ID}" ]; then
            echo "No dataset id found."
            printf "%s\n" "${RESP}" | sed -n '1,200p'
            exit 1
          fi

          curl -sS "https://api.apify.com/v2/datasets/${DATASET_ID}/items?format=csv&clean=true&token=${APIFY_TOKEN}" -o "${OUT_RAW}"
          if [ ! -s "${OUT_RAW}" ]; then
            echo "Downloaded CSV is empty."
            exit 1
          fi

          echo "Raw CSV preview:"
          head -5 "${OUT_RAW}"

      # 2) Normalize: keep only real parent posts, add date_v4, dedupe, keep latest 30, tidy columns
      - name: Normalize CSV for baseline_check.py
        run: |
          python - <<'PY'
          import pandas as pd, numpy as np

          src = "pipeline/latest_posts_raw.csv"
          dst = "pipeline/latest_posts.csv"

          # Read as string to avoid coercions
          df = pd.read_csv(src, dtype=str, keep_default_na=False)

          # Helper to map case-insensitive column names
          lower = {c.lower(): c for c in df.columns}
          def col(*names):
            for n in names:
              if n.lower() in lower:
                return lower[n.lower()]
            return None

          sc_col   = col("shortCode","short_code","shortcode")
          ts_col   = col("timestamp","takenAt","taken_at","published_time")
          url_col  = col("postUrl","url","permalink","link")
          type_col = col("type","productType","media_type")
          cap_col  = col("caption","caption_v4","caption_v3")
          like_col = col("likesCount","likes","like_count","edge_media_preview_like.count")
          com_col  = col("commentsCount","comments","comment_count")
          vvc_col  = col("videoViewCount","video_views","playCount","videoPlayCount")
          disp_col = col("displayUrl","imageUrl","thumbnailUrl","display_url")
          vid_col  = col("videoUrl","video_url")

          def nonempty(s):
            return s.astype(str).str.strip().ne("") if s is not None else pd.Series(False, index=df.index)

          # Keep only rows with a shortcode AND a timestamp (parent posts, not child rows)
          if not sc_col or not ts_col:
            raise SystemExit("Missing required columns for normalization (shortCode/timestamp).")
          mask_parent = nonempty(df[sc_col]) & nonempty(df[ts_col])
          dfp = df.loc[mask_parent].copy()

          # Parse timestamp -> datetime (UTC), derive date_v4
          dt = pd.to_datetime(dfp[ts_col], errors="coerce", utc=True)
          dfp = dfp.loc[dt.notna()].copy()
          dfp["__dt"] = dt.loc[dt.notna()]
          dfp["date_v4"] = dfp["__dt"].dt.strftime("%Y-%m-%d")

          # Build per-shortCode rows, taking first non-empty value per field
          keep_cols = {
            "shortCode": sc_col,
            "postUrl": url_col,
            "caption": cap_col,
            "type": type_col,
            "likesCount": like_col,
            "commentsCount": com_col,
            "videoViewCount": vvc_col,
            "displayUrl": disp_col,
            "videoUrl": vid_col,
            "date_v4": "date_v4",
            "__dt": "__dt",
          }

          def first_nonempty(series):
            for v in series:
              s = str(v) if pd.notna(v) else ""
              if s.strip():
                return s
            return ""

          grouped = []
          for sc, grp in dfp.groupby(sc_col):
            row = {}
            for out, src_col in keep_cols.items():
              if src_col not in grp.columns:
                row[out] = ""
              else:
                row[out] = first_nonempty(grp[src_col])
            # views from videoViewCount if present
            vvc = row.get("videoViewCount","")
            if vvc != "":
              try:
                row["views"] = str(int(float(vvc)))
              except:
                row["views"] = vvc
            else:
              row["views"] = ""
            grouped.append(row)

          clean = pd.DataFrame(grouped)

          # Sort by datetime desc and limit to latest 30
          clean["__dt"] = pd.to_datetime(clean["__dt"], errors="coerce", utc=True)
          clean = clean.sort_values("__dt", ascending=False).head(30).reset_index(drop=True)

          # Final tidy columns (IG-clean style)
          final_cols = ["date_v4","shortCode","postUrl","type","caption",
                        "likesCount","commentsCount","views","displayUrl","videoUrl"]
          for c in final_cols:
            if c not in clean.columns:
              clean[c] = ""
          clean = clean[final_cols]

          clean.to_csv(dst, index=False)
          print("Wrote tidy CSV:", dst)
          print("Columns:", list(clean.columns))
          print(clean.head(10).to_string(index=False))
          PY

      # 3) Create a media manifest (no downloads yet) so you can inspect media columns
      - name: Create media manifest (no downloads yet)
        run: |
          python - <<'PY'
          import pandas as pd, re

          raw = pd.read_csv("pipeline/latest_posts_raw.csv")
          clean = pd.read_csv("pipeline/latest_posts.csv")

          def find(cols, pattern):
            return [c for c in cols if re.search(pattern, c, re.I)]

          url_cols  = find(raw.columns, r'(display|image|thumb|media|video).*url$|^url$')
          type_cols = find(raw.columns, r'(is|media).*video|product.*type|type$')
          id_cols   = find(raw.columns, r'shortcode$|^short[_]?code$|postid$|id$')

          id_col = id_cols[0] if id_cols else None

          media_type = None
          if type_cols:
            t = raw[type_cols[0]].astype(str).str.lower()
            media_type = t.map(lambda x: 'video' if ('true' in x or 'video' in x) else 'image')

          out = {}
          out['post_id']    = raw[id_col] if id_col else pd.Series(range(len(raw)))
          out['media_type'] = media_type if media_type is not None else pd.Series(['unknown']*len(raw))

          for c in url_cols:
            out[c] = raw[c]

          out['shortCode'] = clean.get('shortCode')

          manifest = pd.DataFrame(out)
          manifest.to_csv("pipeline/media_manifest.csv", index=False)

          print("Manifest columns:", list(manifest.columns))
          print("Sample rows:\n", manifest.head(10).to_string(index=False))
          PY

      # 4) Upload artifacts so you can inspect results locally
      - name: Upload raw, normalised & manifest CSVs (artifacts)
        uses: actions/upload-artifact@v4
        with:
          name: apify_csvs_and_manifest
          path: |
            pipeline/latest_posts_raw.csv
            pipeline/latest_posts.csv
            pipeline/media_manifest.csv

      # 5) (Optional) Run your baseline script as before
      - name: Run baseline to Notion
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DB_MASPLUS: ${{ secrets.NOTION_DB_MASPLUS }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          BASELINE_ER_MEDIAN: '0.95'
          BASELINE_IVR_MEDIAN: '3.09'
          HC_INPUT_CSV: pipeline/latest_posts.csv
          HC_FOLLOWERS: '485000'
        run: python pipeline/baseline_check.py
