name: weekly-insights

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * 1'

permissions:
  contents: read

concurrency:
  group: weekly-insights-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  weekly_job:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas python-dateutil python-dotenv requests numpy tenacity openai==1.51.2
          pip install yt-dlp
          sudo apt-get update
          sudo apt-get install -y ffmpeg

      # 1) Fetch from Apify (async)
      - name: Fetch MAS+ data from Apify (async poll)
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p pipeline
          OUT_RAW="pipeline/latest_posts_raw.csv"
          RUN_JSON="pipeline/apify_run.json"

          JSON_PAYLOAD='{"username":["masbymessi"],"maxItems":100,"proxy":{"useApifyProxy":true}}'

          curl -sS -X POST \
            -H "Authorization: Bearer ${APIFY_TOKEN}" \
            -H "Content-Type: application/json" \
            "https://api.apify.com/v2/acts/apify~instagram-post-scraper/runs?token=${APIFY_TOKEN}" \
            -d "${JSON_PAYLOAD}" \
            -o "${RUN_JSON}"

          RUN_ID=$(python -c 'import json,sys; d=json.load(open(sys.argv[1])); print(d.get("data",{}).get("id",""))' "${RUN_JSON}")
          if [ -z "${RUN_ID}" ]; then
            echo "Failed to start Apify run"
            sed -n '1,200p' "${RUN_JSON}" || true
            exit 1
          fi

          while :; do
            RESP=$(curl -sS -H "Authorization: Bearer ${APIFY_TOKEN}" "https://api.apify.com/v2/actor-runs/${RUN_ID}")
            STATUS=$(printf '%s' "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read() or "{}"); print(d.get("data",{}).get("status",""))')
            echo "status: ${STATUS}"
            case "${STATUS}" in
              SUCCEEDED) break;;
              FAILED|TIMED-OUT|ABORTED) printf '%s\n' "${RESP}"; exit 1;;
            esac
            sleep 30
          done

          DATASET_ID=$(printf '%s' "${RESP}" | python -c 'import json,sys; d=json.loads(sys.stdin.read() or "{}"); print(d.get("data",{}).get("defaultDatasetId",""))')
          if [ -z "${DATASET_ID}" ]; then
            echo "No dataset id"
            printf '%s\n' "${RESP}"
            exit 1
          fi

          curl -sS "https://api.apify.com/v2/datasets/${DATASET_ID}/items?format=csv&clean=true&token=${APIFY_TOKEN}" -o "${OUT_RAW}"
          if [ ! -s "${OUT_RAW}" ]; then
            echo "Empty CSV"
            exit 1
          fi
          head -5 "${OUT_RAW}"

      # 2) Normalize -> last 7 days
      - name: Normalize CSV for baseline_check.py
        run: |
          python - <<'PY'
          import pandas as pd, numpy as np, re
          from pandas import Timestamp, Timedelta

          src="pipeline/latest_posts_raw.csv"; dst="pipeline/latest_posts.csv"
          df=pd.read_csv(src,dtype=str,keep_default_na=False)
          lower={c.lower():c for c in df.columns}
          def col(*names):
              for n in names:
                  if n and n.lower() in lower: return lower[n.lower()]
              return None

          sc_col=col("shortCode","short_code","shortcode")
          posturl_col=col("postUrl","url","permalink","link")
          if not sc_col and posturl_col:
              def extract_sc(u):
                  m=re.search(r"/(?:p|reel)/([A-Za-z0-9_-]+)/?", str(u)); return m.group(1) if m else ""
              df["__sc_from_url"]=df[posturl_col].map(extract_sc); sc_col="__sc_from_url"

          ts_candidates=["timestamp","takenAt","taken_at","takenAtISO","taken_at_iso","takenAtUtc","taken_at_utc",
                         "takenAtTimestamp","taken_at_timestamp","created_time","createdAt","published_time","date","time","datetime"]
          ts_cols=[c for c in (col(x) for x in ts_candidates) if c]

          def parse_ts_row(row):
              for c in ts_cols:
                  val=str(row.get(c,"")).strip()
                  if not val: continue
                  dt=pd.to_datetime(val,errors="coerce",utc=True)
                  if pd.notna(dt): return dt
                  num=pd.to_numeric(val,errors="coerce")
                  if pd.notna(num):
                      for unit in ["s","ms"]:
                          dt=pd.to_datetime(num,unit=unit,errors="coerce",utc=True)
                          if pd.notna(dt): return dt
              return pd.NaT

          df["_sc"]= (df[sc_col] if sc_col else "").astype(str).str.strip()
          df["_postUrl"]= df[posturl_col] if posturl_col else ""
          df["_dt"]= df.apply(parse_ts_row,axis=1)

          now = Timestamp.now(tz="UTC"); week_cut = now - Timedelta(days=7)
          df = df.loc[df["_dt"].notna() & (df["_dt"] >= week_cut)].copy()

          parent=df.loc[df["_sc"].ne("")].copy()
          if parent.empty:
              pd.DataFrame(columns=["date_v4","shortCode","postUrl","type","caption","likesCount","commentsCount","views","displayUrl","videoUrl"]).to_csv(dst,index=False)
              raise SystemExit(0)

          parent["date_v4"]=parent["_dt"].dt.strftime("%Y-%m-%d")

          def first_nonempty(s):
              for v in s:
                  sv=str(v) if pd.notna(v) else ""
                  if sv.strip(): return sv
              return ""

          def colopt(*names):
              c=col(*names); return c if (c in parent.columns) else None

          type_col=colopt("type","productType","media_type")
          cap_col =colopt("caption","caption_v4","caption_v3")
          like_col=colopt("likesCount","likes","like_count","edge_media_preview_like.count")
          com_col =colopt("commentsCount","comments","comment_count")
          vvc_col =colopt("videoViewCount","video_views","playCount","videoPlayCount")
          disp_col=colopt("displayUrl","imageUrl","thumbnailUrl","display_url")
          vid_col =colopt("videoUrl","video_url")

          rows=[]
          for sc,grp in parent.groupby("_sc"):
              row={"shortCode":sc,
                   "postUrl":first_nonempty(grp["_postUrl"]),
                   "caption":first_nonempty(grp[cap_col]) if cap_col else "",
                   "type":first_nonempty(grp[type_col]) if type_col else "",
                   "likesCount":first_nonempty(grp[like_col]) if like_col else "",
                   "commentsCount":first_nonempty(grp[com_col]) if com_col else "",
                   "displayUrl":first_nonempty(grp[disp_col]) if disp_col else "",
                   "videoUrl":first_nonempty(grp[vid_col]) if vid_col else "",
                   "date_v4":first_nonempty(grp["date_v4"]),
                   "__dt":grp["_dt"].max()}
              if vvc_col:
                  try:
                      vv=pd.to_numeric(grp[vvc_col].replace("",np.nan),errors="coerce").max()
                      row["views"]= "" if pd.isna(vv) else str(int(vv))
                  except: row["views"]=first_nonempty(grp[vvc_col])
              else: row["views"]=""
              rows.append(row)

          clean=pd.DataFrame(rows)
          clean["__dt"]=pd.to_datetime(clean["__dt"],errors="coerce",utc=True)
          clean=clean.sort_values("__dt",ascending=False).reset_index(drop=True)

          final_cols=["date_v4","shortCode","postUrl","type","caption","likesCount","commentsCount","views","displayUrl","videoUrl"]
          for c in final_cols:
              if c not in clean.columns: clean[c]=""
          clean=clean[final_cols]
          clean.to_csv(dst,index=False)
          print("Weekly rows:", len(clean))
          PY

      # 5) Notion with Linked Post(s) fallback and fixed Debug Confidence
      - name: Publish weekly visual narrative to Notion
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DB_MASPLUS: ${{ secrets.NOTION_DB_MASPLUS }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, json, pandas as pd, requests
          from datetime import datetime, timezone

          NOTION_TOKEN=os.environ["NOTION_TOKEN"]
          DB_ID=os.environ["NOTION_DB_MASPLUS"]
          H={"Authorization":f"Bearer {NOTION_TOKEN}","Notion-Version":"2022-06-28","Content-Type":"application/json"}

          def rt(x):  return {"rich_text":[{"type":"text","text":{"content":str(x)[:1900]}}]}
          def ttl(x): return {"title":[{"type":"text","text":{"content":str(x)[:200]}}]}
          def sel(x): return {"select":{"name":x}}
          def msel(tags): return {"multi_select":[{"name":t[:100]} for t in tags if t]}

          posts=pd.read_csv("pipeline/latest_posts.csv") if os.path.exists("pipeline/latest_posts.csv") else pd.DataFrame()
          week=datetime.now(timezone.utc).strftime("%Y-%m-%d")
          n=len(posts)

          motif_counts=posts.get("Motif_v1",pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
          terr_counts =posts.get("Assigned_Territory_v4",pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
          motif_line=" • ".join([f"{k}: {v}" for k,v in motif_counts.items()]) if motif_counts else "No motifs"
          terr_line =" • ".join([f"{k}: {v}" for k,v in terr_counts.items()])  if terr_counts  else "No territories"
          metric_text=f"Posts: {n} | Motifs: {motif_line} | Territories: {terr_line}"

          avg_conf=float(posts.get("Motif_conf",pd.Series([0.5]*n)).mean()) if n else 0.5
          conf_band="High" if avg_conf>=0.75 else ("Medium" if avg_conf>=0.5 else "Low")

          motifs=[m for m in posts.get("Motif_v1",[]) if isinstance(m,str)]
          intents=[i for i in posts.get("Caption_Intent_v1",[]) if isinstance(i,str)]
          driver_tags=sorted(set(motifs+intents))

          def row_line(r): return f"{r.get('date_v4','?')} · {r.get('shortCode','?')} · {r.get('postUrl','')}"
          linked_posts_list="\n".join([row_line(r) for _,r in posts.iterrows()]) if n else ""

          first_url=""
          if n:
            for _,r in posts.iterrows():
              u=str(r.get("postUrl","") or "")
              if u.startswith("http"):
                first_url=u
                break

          # debug confidence safe join
          dbg_list=[f"{r.get('shortCode','?')}: {r.get('Motif_conf',0)}" for _,r in posts.iterrows()] if n else []
          dbg_str="\n".join(dbg_list)

          payload_url={
            "parent":{"database_id":DB_ID},
            "properties":{
              "Date":{"date":{"start":week}},
              "Insight Type":sel("Weekly Motifs"),
              "Title":ttl("Weekly Motifs & Territories"),
              "Headline":rt(f"Weekly Motifs & Territories — {week}"),
              "Metric":rt(metric_text),
              "Confidence":sel(conf_band),
              "Driver Tags":msel(driver_tags),
              "Linked Post(s)":{"url": first_url},
              "Action":rt("Balance reach (Crowd/Creative Momentum) with depth (Macro/Performance)."),
              "Impact Score":{"number":round(min(1.0, n/10.0)*avg_conf*100)},
              "Debug Confidence":rt(dbg_str),
              "Status":sel("Published")
            },
            "children":[
              {"object":"block","type":"paragraph",
               "paragraph":{"rich_text":[{"type":"text","text":{"content":metric_text}}]}}
            ]
          }

          r=requests.post("https://api.notion.com/v1/pages", headers=H, data=json.dumps(payload_url))
          if r.status_code==400 and "expected to be url" in r.text.lower():
            props_rt = payload_url["properties"].copy()
            props_rt["Linked Post(s)"] = rt(linked_posts_list)
            payload_rt = {"parent":{"database_id":DB_ID},"properties":props_rt,"children":payload_url["children"]}
            r = requests.post("https://api.notion.com/v1/pages", headers=H, data=json.dumps(payload_rt))

          print("Notion:", r.status_code, r.text[:300])
          PY

      - name: Upload weekly artifacts
        uses: actions/upload-artifact@v4
        with:
          name: weekly_csvs_videos_frames
          path: |
            pipeline/latest_posts_raw.csv
            pipeline/latest_posts.csv
            media_video
            frames_video
