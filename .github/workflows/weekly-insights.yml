name: weekly-insights

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * 1'   # Mondays 06:00 UTC

permissions:
  contents: read

jobs:
  run:
    runs-on: ubuntu-latest

    defaults:
      run:
        shell: bash

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas python-dateutil python-dotenv requests openai==1.51.2 tenacity

      # ---- Fetch last 7 days of MAS+ posts from Apify (actor, no task id) ----
            - name: Fetch latest MAS+ data from Apify
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p pipeline
          OUT="pipeline/latest_posts.csv"
          BODY="pipeline/apify_body.out"
          CODE="pipeline/apify_code.txt"
          HDRS="pipeline/apify_headers.txt"

          # Minimal, valid input for apify~instagram-post-scraper
          JSON_PAYLOAD=$(cat <<'JSON'
          {
            "usernames": ["masbymessi"],
            "resultsLimit": 30,
            "proxy": { "useApifyProxy": true }
          }
          JSON
          )

          # Call actor and capture everything cleanly
          HTTP_CODE=$(curl -sS \
            -H "Authorization: Bearer ${APIFY_TOKEN}" \
            -H "Content-Type: application/json" \
            -X POST \
            -D "$HDRS" \
            "https://api.apify.com/v2/acts/apify~instagram-post-scraper/run-sync-get-dataset-items?format=csv&clean=true&token=${APIFY_TOKEN}" \
            -d "$JSON_PAYLOAD" \
            -o "$OUT" \
            -w "%{http_code}")

          echo "$HTTP_CODE" > "$CODE"

          if [ "$HTTP_CODE" != "200" ]; then
            # Re-run without file output to capture the JSON error body for debugging
            curl -sS \
              -H "Authorization: Bearer ${APIFY_TOKEN}" \
              -H "Content-Type: application/json" \
              -X POST \
              "https://api.apify.com/v2/acts/apify~instagram-post-scraper/run-sync-get-dataset-items?format=json&clean=true&token=${APIFY_TOKEN}" \
              -d "$JSON_PAYLOAD" > "$BODY" || true

            echo "Apify returned HTTP $HTTP_CODE"
            echo "Request payload:"
            echo "$JSON_PAYLOAD"
            echo "---- Response headers ----"
            sed -n '1,200p' "$HDRS" || true
            echo "---- Response body ----"
            sed -n '1,400p' "$BODY" || true
            exit 1
          fi

          echo "First lines of downloaded CSV:"
          head -5 "$OUT" || true

          # Optional: ensure file is not empty
          if [ ! -s "$OUT" ]; then
            echo "Downloaded CSV is empty."
            exit 1
          fi

      # ---- Analyse → classify → post to Notion ----
      - name: Run baseline → Notion
        env:
          NOTION_TOKEN:       ${{ secrets.NOTION_TOKEN }}        # secret_...
          NOTION_DB_MASPLUS:  ${{ secrets.NOTION_DB_MASPLUS }}   # 32-char DB id
          OPENAI_API_KEY:     ${{ secrets.OPENAI_API_KEY }}      # sk-...
          BASELINE_ER_MEDIAN: '0.95'
          BASELINE_IVR_MEDIAN: '3.09'
          HC_INPUT_CSV:       pipeline/latest_posts.csv
          HC_FOLLOWERS:       '485000'
        run: python pipeline/baseline_check.py
